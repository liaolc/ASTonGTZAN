{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yAGr4_8WRgR"
      },
      "source": [
        "Script to trim one of the 4 hour files I grabbed for negatives to take about 220 10 second segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijvx-y8BWQc2",
        "outputId": "60840573-3e52-46fd-8ea5-a007ec0416a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def trim_audio(input_file, start_time, duration, output_dir):\n",
        "    # Load audio file\n",
        "    audio = AudioSegment.from_wav(input_file)\n",
        "\n",
        "    # Convert start time and duration to milliseconds\n",
        "    start_time = start_time * 1000\n",
        "    duration = duration * 1000\n",
        "\n",
        "    # Trim audio\n",
        "    trimmed_audio = audio[start_time:start_time + duration]\n",
        "\n",
        "    # Define output filename based on input filename\n",
        "    base_filename = os.path.basename(input_file)\n",
        "    output_file = os.path.join(output_dir, base_filename)\n",
        "\n",
        "    # Save trimmed audio\n",
        "    trimmed_audio.export(output_file, format=\"wav\")\n",
        "\n",
        "# Input file path\n",
        "input_file = \"ExtraNegative/20221208_180000.WAV\"\n",
        "\n",
        "# Start time in seconds (1 hour 30 minutes = 5400 seconds)\n",
        "start_time = 5400\n",
        "\n",
        "# Duration in seconds\n",
        "duration = 2200\n",
        "\n",
        "# Output directory\n",
        "output_dir = \"ExtraNegative\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "trim_audio(input_file, start_time, duration, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QIgXlOX0IN4"
      },
      "source": [
        "This next cell will split given audio files of any length into 10 second segments, and save them to the specified output directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzD23XKRjudJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "def split_all_audio_files(input_dir, output_dir, segment_length_sec=10):\n",
        "    # Make sure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Loop over all files in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        # Check if the file is a .wav file\n",
        "        if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "            try:\n",
        "                # Full path to the original audio file\n",
        "                audio_path = os.path.join(input_dir, filename)\n",
        "                # Load the audio file\n",
        "                waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "                # Calculate number of samples in segment_length_sec\n",
        "                num_samples_segment = segment_length_sec * sample_rate\n",
        "\n",
        "                # Calculate total number of segments\n",
        "                total_segments = math.ceil(waveform.shape[1] / num_samples_segment)\n",
        "\n",
        "                # Split waveform into segments and save each segment to a new .wav file\n",
        "                for i in range(total_segments):\n",
        "                    start = i * num_samples_segment\n",
        "                    end = start + num_samples_segment\n",
        "                    segment = waveform[:, start:end]\n",
        "\n",
        "                    # Prepare filename for the segment\n",
        "                    segment_filename = f\"{filename.rstrip('.wav')}_segment{i}.wav\"\n",
        "                    segment_path = os.path.join(output_dir, segment_filename)\n",
        "\n",
        "                    # Save segment as a .wav file\n",
        "                    segment = (segment * 32767).short()  # Convert to 16-bit PCM format\n",
        "                    torchaudio.save(segment_path, segment, sample_rate)\n",
        "            except Exception as e:\n",
        "                  print(f\"Error processing file {audio_path}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHy6iYlR0klJ"
      },
      "outputs": [],
      "source": [
        "input_dir = 'ExtraPositive'\n",
        "output_dir = 'ExtraPositive'\n",
        "split_all_audio_files(input_dir, output_dir, segment_length_sec=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boJgdt6BXYuy"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "zip and download split files to local. or use google drive\n",
        "\n",
        "files stored on google colab do not persist. I will download these files to review the positives again as some segments will no longer have our call after splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6xBv1zHXw-d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/SplitNegative.zip /content/SplitNegative\n",
        "#!zip -r /content/SplitPositive.zip /content/SplitPositive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PvOokaCPYRg1",
        "outputId": "0a76e08f-0f63-4c36-a10e-4a83144a2226"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_594c0a4d-78ed-4d7c-b024-ae85781929b7\", \"SplitNegative.zip\", 268831461)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(\"/content/SplitNegative.zip\")\n",
        "#files.download(\"/content/SplitPositive.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gflv71Ovl2kZ"
      },
      "source": [
        "After reviewing the files, I placed them in a google drive. Gain access here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ2OYyNfD0K1",
        "outputId": "3884ac98-44fc-4437-9e74-7c7c0df8d22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n",
            "[Errno 2] No such file or directory: 'Rana_Draytonii/ # Location of data'\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive/\n",
        "%cd Rana_Draytonii/ # Location of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY0QEgZfN3LB",
        "outputId": "eeebf2d5-e26e-41e4-8115-123243a52a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Rana_Draytonii\n"
          ]
        }
      ],
      "source": [
        "%cd Rana_Draytonii/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOeHSJ-ZEwsa",
        "outputId": "0918e044-d927-4564-f2f4-18421f5b412c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m'Negative samples'\u001b[0m/   \u001b[01;34mRana3\u001b[0m/   \u001b[01;34mReducedNegative\u001b[0m/   ResultRana7.zip\n",
            "\u001b[01;34m'Positive samples'\u001b[0m/   \u001b[01;34mRana4\u001b[0m/   \u001b[01;34mReducedPositive\u001b[0m/   \u001b[01;34mSplitNegative\u001b[0m/\n",
            " \u001b[01;34mRana1\u001b[0m/               \u001b[01;34mRana7\u001b[0m/   \u001b[01;34mResampledAudio\u001b[0m/    \u001b[01;34mSplitPositive\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YZm8eeHR8rW",
        "outputId": "ac530991-3b9d-47c5-c3c3-17c183a4c5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# Check how many CPU cores are available\n",
        "!cat /proc/cpuinfo | grep 'processor' | wc -l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gES2lpdAx9U5"
      },
      "source": [
        "**Reduce frequency range of files**\n",
        "\n",
        "This takes way too long, I will need to find a better way of doing this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV3aqp0ATNjW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def process_file(args):\n",
        "    audio_path, save_dir = args\n",
        "    # Check for '.wav' or '.WAV' file extensions here\n",
        "    if audio_path.endswith('.wav') or audio_path.endswith('.WAV'):\n",
        "        new_filepath = process_audio(audio_path, save_dir)\n",
        "        print(f\"File {new_filepath} finished\")\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def bandpass_filter(buffer):\n",
        "    return butter_bandpass_filter(buffer, lowcut, highcut, FRAME_RATE, order=6)\n",
        "\n",
        "def process_audio(audio_path, save_dir):\n",
        "    # create new filename\n",
        "    base_filename = os.path.basename(audio_path)\n",
        "    new_filename = os.path.splitext(base_filename)[0] + '_reduced.wav'\n",
        "    new_path = os.path.join(save_dir, new_filename)\n",
        "\n",
        "    # Skip if file already processed\n",
        "    if os.path.exists(new_path):\n",
        "        return new_path\n",
        "\n",
        "    # load your audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # Convert tensor to numpy array for bandpass filter\n",
        "    waveform_np = waveform.numpy()\n",
        "\n",
        "    # Apply bandpass filter\n",
        "    waveform_np = np.apply_along_axis(bandpass_filter, 0, waveform_np)\n",
        "\n",
        "    # Normalize the waveform if necessary\n",
        "    if waveform_np.min() < -1.0 or waveform_np.max() > 1.0:\n",
        "        waveform_np = waveform_np / np.max(np.abs(waveform_np))\n",
        "\n",
        "    # Convert back to tensor and ensure the type is float32\n",
        "    waveform = torch.from_numpy(waveform_np).float()\n",
        "\n",
        "    # save the resampled audio\n",
        "    torchaudio.save(new_path, waveform, sample_rate)\n",
        "\n",
        "    return new_path\n",
        "\n",
        "# For use with bandpass filter\n",
        "lowcut = 10.0\n",
        "highcut = 3000.0\n",
        "FRAME_RATE = 16000\n",
        "\n",
        "# Define the path where your positive and negative .wav files are stored\n",
        "positive_audio_path = 'SplitPositive'\n",
        "negative_audio_path = 'SplitNegative'\n",
        "# Define directories to save updated audio files (0 - 3kHz)\n",
        "positive_reduced_audio_dir = 'ReducedPositive'\n",
        "negative_reduced_audio_dir = 'ReducedNegative'\n",
        "\n",
        "# Create a list of arguments for process_file\n",
        "positive_args = [(os.path.join(positive_audio_path, filename), positive_reduced_audio_dir)\n",
        "                 for filename in os.listdir(positive_audio_path)]\n",
        "negative_args = [(os.path.join(negative_audio_path, filename), negative_reduced_audio_dir)\n",
        "                 for filename in os.listdir(negative_audio_path)]\n",
        "args = positive_args + negative_args\n",
        "\n",
        "# Create a pool of workers\n",
        "with Pool(processes=2) as pool:  # adjust based on number of CPU cores\n",
        "    # Map process_file function over all arguments\n",
        "    pool.map(process_file, args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn-H_dDc2yWL"
      },
      "source": [
        "**Resample to correct frequency, apply labels, and create label.csv and json files**\n",
        "\n",
        "\n",
        "\n",
        "**Before Running:**\n",
        "\n",
        "\n",
        "1.   specify path to positive and negative samples\n",
        "2.   create folder and specify path for resampled audio\n",
        "**Prepare JSON files and labels.csc for datasets:**\n",
        "\n",
        "This script creates a JSON file for each of the train, validation, and test sets, each named train_data.json, val_data.json, and test_data.json, respectively.\n",
        "\n",
        "Each entry in the JSON file will contain the path to the audio file, and the corresponding label.\n",
        "\n",
        "The script will also resample the audio files, and convert and stereo files to mono.\n",
        "\n",
        "This could be run in the greater Rana_Draytonii folder, then these files and the ResampledAudio folder should be moved into the Rana7 folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvZu8OUwxFbl"
      },
      "source": [
        "**Delete all files in ResampledAudio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "broFc_BxxJ53"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Directory to clear\n",
        "resampled_audio_dir = 'ResampledAudio'\n",
        "\n",
        "# Delete all files in the directory\n",
        "for filename in os.listdir(resampled_audio_dir):\n",
        "    file_path = os.path.join(resampled_audio_dir, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Klxj4wZqDWU",
        "outputId": "4c5ae0dc-07bd-43b7-c2ed-b8914c375135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Key 'Positive' found in index_dict. Corresponding value is /m/positive\n",
            "Key 'Negative' found in index_dict. Corresponding value is /m/negative\n",
            "index_dict: {'Positive': '/m/positive', 'Negative': '/m/negative'}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import csv\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "# Changes sampling frequency of audio file to 16kHz required by the AST model\n",
        "def resampler(audio_path, save_dir):\n",
        "    # load your audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # define resampler\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "\n",
        "    # resample the waveform\n",
        "    waveform_resampled = resampler(waveform)\n",
        "\n",
        "    # create new filename\n",
        "    base_filename = os.path.basename(audio_path)\n",
        "    new_filename = os.path.splitext(base_filename)[0] + '_resampled.wav'\n",
        "    new_path = os.path.join(save_dir, new_filename)\n",
        "\n",
        "    # save the resampled audio\n",
        "    torchaudio.save(new_path, waveform_resampled, sample_rate=16000)\n",
        "\n",
        "    return new_path\n",
        "\n",
        "# Function to create an index dictionary file per model specifications\n",
        "def make_index_dict(label_csv):\n",
        "    index_lookup = {}\n",
        "    with open(label_csv, 'r') as f:\n",
        "        csv_reader = csv.DictReader(f)\n",
        "        for row in csv_reader:\n",
        "            index_lookup[row['display_name']] = row['mid']\n",
        "    return index_lookup\n",
        "\n",
        "import json\n",
        "\n",
        "# Function to create .json file with filenames and labels per model specifications\n",
        "def create_data_json(dataset, labels, filename, index_dict):\n",
        "    data = []\n",
        "    for wav_path, label in zip(dataset, labels):\n",
        "        entry = {\n",
        "            \"wav\": wav_path,\n",
        "            \"labels\": index_dict[label],\n",
        "        }\n",
        "        data.append(entry)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump({\"data\": data}, f, indent=4)\n",
        "\n",
        "# Function to convert strero files to mono\n",
        "def stereo_to_mono(directory_path):\n",
        "  # Loop through all files in the directory\n",
        "  for filename in os.listdir(directory_path):\n",
        "      # Check if the file is a .wav file\n",
        "      if filename.endswith('.wav'):\n",
        "          # Get the full path of the file\n",
        "          file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "          # Load audio file\n",
        "          audio = AudioSegment.from_wav(file_path)\n",
        "\n",
        "          # If the audio file is stereo\n",
        "          if audio.channels == 2:\n",
        "              print(f\"Converting stereo file: {filename}\")\n",
        "\n",
        "              # Convert to mono\n",
        "              mono_audio = audio.set_channels(1)\n",
        "\n",
        "              # Replace the original file with the mono version\n",
        "              mono_audio.export(file_path, format='wav')\n",
        "\n",
        "\n",
        "# Define the path where your positive and negative .wav files are stored\n",
        "positive_audio_path = 'SplitPositive'\n",
        "negative_audio_path = 'SplitNegative'\n",
        "# Define a directory to save resampled audio files (16kHz)\n",
        "resampled_audio_dir = 'ResampledAudio'\n",
        "\n",
        "# Define the target length for your spectrograms (only used for mel spectrogram)\n",
        "target_length = 1000\n",
        "mel_bins = 128  # Number of bins in Mel spectrogram\n",
        "\n",
        "# Define labels\n",
        "positive_label = 0\n",
        "negative_label = 1\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = []\n",
        "numeric_labels = []  # For train_test_split and torch.Tensor\n",
        "string_labels = []  # For JSON file\n",
        "\n",
        "# Convert to mono\n",
        "stereo_to_mono(positive_audio_path)\n",
        "stereo_to_mono(negative_audio_path)\n",
        "\n",
        "# Process positive samples\n",
        "for filename in os.listdir(positive_audio_path):\n",
        "    if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "        filepath = os.path.join(positive_audio_path, filename)\n",
        "        filepath = resampler(filepath, resampled_audio_dir)  # Resample and get new file path\n",
        "        dataset.append(filepath)  # Save filepath instead of spectrogram\n",
        "        numeric_labels.append(positive_label)\n",
        "        string_labels.append('Positive')\n",
        "\n",
        "# Process negative samples\n",
        "for filename in os.listdir(negative_audio_path):\n",
        "    if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "        filepath = os.path.join(negative_audio_path, filename)\n",
        "        filepath = resampler(filepath, resampled_audio_dir)  # Resample and get new file path\n",
        "        dataset.append(filepath)  # Save filepath instead of spectrogram\n",
        "        numeric_labels.append(negative_label)\n",
        "        string_labels.append('Negative')\n",
        "\n",
        "numeric_labels = torch.Tensor(numeric_labels)\n",
        "\n",
        "# train_test_split\n",
        "dataset_trainval, dataset_test, labels_trainval, labels_test, string_labels_trainval, string_labels_test = train_test_split(dataset, numeric_labels, string_labels, test_size=0.15, random_state=42, stratify=numeric_labels)\n",
        "dataset_train, dataset_val, labels_train, labels_val, string_labels_train, string_labels_val = train_test_split(dataset_trainval, labels_trainval, string_labels_trainval, test_size=0.15, random_state=42, stratify=labels_trainval)\n",
        "\n",
        "# Create labels.csv\n",
        "labels = {\n",
        "    'index': [0, 1],  # Modify the index values as per your labels\n",
        "    'mid': ['/m/positive', '/m/negative'],  # Modify the MID values as per your labels\n",
        "    'display_name': ['Positive', 'Negative']  # Modify the display names as per your labels\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(labels)\n",
        "df.to_csv('labels.csv', index=False)\n",
        "\n",
        "index_dict = make_index_dict('labels.csv')\n",
        "\n",
        "# Check if the keys you'll use exist in the dictionary (testing)\n",
        "expected_keys = ['Positive', 'Negative']\n",
        "for key in expected_keys:\n",
        "    if key not in index_dict:\n",
        "        print(f\"Key '{key}' not found in index_dict\")\n",
        "    else:\n",
        "        print(f\"Key '{key}' found in index_dict. Corresponding value is {index_dict[key]}\")\n",
        "print(\"index_dict:\", index_dict)\n",
        "\n",
        "\n",
        "# Create json files\n",
        "create_data_json(dataset_train, string_labels_train, 'train_data.json', index_dict)\n",
        "create_data_json(dataset_val, string_labels_val, 'val_data.json', index_dict)\n",
        "create_data_json(dataset_test, string_labels_test, 'test_data.json', index_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNXuEEdgKDdu",
        "outputId": "58c65a50-7b87-495e-9a4c-8d47a2c206ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Rana_Draytonii/Rana7\n"
          ]
        }
      ],
      "source": [
        "%cd Rana7/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}